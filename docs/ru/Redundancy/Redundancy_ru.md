**Руководство**: **Создание и настройка нового кластера Faceplate**

Данный документ описывает процесс развертывания нового кластера Faceplate (Runtime и Historian), его первоначальную настройку и тюнинг производительности.

![Пример кластера](/Redundancy/FP_cluster.png)

Пример класстера Faceplate

1\. **Подготовка инфраструктуры**

1.1. **Требования к серверам**

\- Операционная система: **Ubuntu Server 22.04.05 LTS и выше.**

\- Сеть: Для локального обмена данными между серверами рекомендуется выделить локальную подсеть (например, 10.230.0.0/24), обеспечивающую канал связи до 1 Гб/c и выше.

1.2. **Настройка DNS и /etc/hosts**

Ключевым фактором работы кластера является корректная настройка локальных имен.

1\. Откройте файл /etc/hosts на каждом сервере.

2\. Пропишите IP-адреса и локальные имена всех серверов, которые будут входить в кластер.

Важно: Содержимое /etc/hosts (секция с именами нод Faceplate) должно быть абсолютно идентичным на всех серверах кластера. Без этого обмен данными между нодами невозможен.

Пример конфигурации:

**10.230.0.11 rt-server1.fp**

**10.230.0.12 rt-server2.fp**

**10.230.0.13 his-server1.fp**

2\. **Установка и структура приложения**

2.1. **Размещение файлов**

Приложение должно быть развернуто в следующей структуре каталогов:

\- /DATASET/project/scada/faceplate/ - файлы самого приложения (Runtime/Historian).

\- /DATASET/project/scada/local/ - база данных и локальные настройки.

\- /DATASET/project/scada/local/logs/ - файлы журналов (console.log, error.log).

2.2. **Сертификаты**

Для работы HTTPS соединений разместите файлы сертификатов в директории **/DATASET/project/scada/faceplate/etc/certs**:

**\- ca-chain.cert.pem**

**\- node.cert.pem**

**\- node.key.pem**

3\. **Базовая конфигурация (Tuning)**

Перед запуском необходимо настроить ключевые конфигурационные файлы, расположенные в **/DATASET/project/scada/faceplate/releases/4.0.6/** (и подпапке **apps/**).

3.1. **vm.args** (Настройки виртуальной машины **Erlang**)

В этом файле необходимо задать уникальные параметры для каждой ноды:

\- -**name**: Укажите уникальное имя ноды (например, <fp@rt-server1.fp>).

\- -**setcookie**: Задайте единый cookie для всех серверов в кластере. Это пароль для взаимодействия нод.

\- **+MIscs**: Флаг для лимитирования памяти литерала (настраивается в мегабайтах).

3.2. **fp.logger.config** (Настройка логирования)

Настройте пути для вывода логов в блоках console_handler и error_handler. Убедитесь, что пути указывают на корректную папку, например:

**\- /DATASET/project/scada/local/logs/console.log**

**\- /DATASET/project/scada/local/logs/error.log**

4\. **Запуск и сборка кластера**

Запуск производится в сессии tmux (имя сессии scada).

4.1. **Запуск первой** **ноды**

На первом сервере выполните команду запуска в консольном режиме:

./faceplate console

4.2. **Добавление остальных нод в кластер**

Для подключения последующих серверов (Runtime 2, Runtime 3 или Historian) к кластеру, команду запуска нужно выполнить один раз с переменной окружения **ATTACH_TO**. Она указывает на уже работающую ноду.

Команда для добавления ноды:

**env ATTACH_TO=<fp@rt-server1.fp> ./faceplate console** (Где **<fp@rt-server1.fp>** - имя первой запущенной ноды). После успешного добавления в кластер, при последующих перезапусках используйте обычную команду **./faceplate console**.

5\. Настройка хранилища (**Faceplate Studio**)

После запуска кластера необходимо настроить базы данных через Faceplate Studio (<http://xxx.xxx.xxx.xxx:90000/fp/studio>).

Настройки делятся на три категории.

5.1. **Runtime** (Оперативные данные)

Конфигурация для хранения значений тегов в реальном времени.

\- **Модули**: Рекомендуется использовать комбинацию **RAM** и диска

(**zaya_ets_rocksdb**) или только диск (**zaya_rocksdb**).

Необходимо чтобы количество нод и параметры в «Настройка хранилища» соответствовали, ниже указанный конфиг дублируется на количество нод

```
":atom:fp@uzn-server1.fp": {":atom:disc": {":atom:module": ":atom:zaya_rocksdb",  
<br/>":atom:params": {}  
<br/>},  
<br/>":atom:ram": {  
<br/>":atom:module": ":atom:zaya_ets",  
<br/>":atom:params": {}  
<br/>},  
<br/>":atom:ramdisc": {  
<br/>":atom:module": ":atom:zaya_ets_rocksdb",  
<br/>":atom:params": {}  
<br/>}  
```
5.2. **Сообщения** (Журналы и алармы)

\- max_age: Установите период хранения алармов (в днях).

\- shard_depth: Задайте временной диапазон для одного шарда (файла базы данных) в днях.

\- read_only_age: Возраст данных, после которого архив переходит в режим "только чтение".

5.3. **Архивы**

Настройки для долговременного архива (обычно на базе RocksDB):

\- **remove_age**: Глубина хранения архива (например, 365 дней).

\- **buffer_limit**: Лимит буфера записи (например, 1 ГБ).

\- **open_options**: Настройки производительности (размер блока 32 КБ, сжатие lz4, асинхронная запись).

6.2. **Роли и лимиты памяти**

В разделе "Права" настройте пользователей и их лимиты потребления памяти:

\- Операторы (мониторинг): Рекомендуемый лимит ~4000 МБ.

\- Инженеры (активная работа, выгрузка логов): Рекомендуемый лимит ~8000 МБ.

6.3. **Запуск в режиме сервиса**

После отладки прикладного проекта предполагается переход на запуск Faceplate в режиме сервиса

Предлагается интерактивный установщик

Копируем скрипт в новый файл распологаем рядом с исходням дистрибутиом называем «install_fp_service.sh» применям команду chmod +x install_fp_service.sh т.е делаем его исполняемым и запускаем ./install_fp_service.sh далее следуем интерактивному режуму скрипта
```
# !/bin/bash

\# ==========================================

\#  Интерактивный установщик 'foreground' сервиса (БЕЗ ЛОГОВ)

\# ==========================================

\# --- Цвета для вывода ---

RED='\\033\[0;31m'

GREEN='\\033\[0;32m'

BLUE='\\033\[0;34m'

YELLOW='\\033\[1;33m'

NC='\\033\[0m' # No Color

\# --- Функции ---

print_info() {

&nbsp;   echo -e "\\n\${BLUE}INFO:\${NC} \$1"

}

print_success() {

&nbsp;   echo -e "\${GREEN}SUCCESS:\${NC} \$1"

}

print_error() {

&nbsp;   echo -e "\${RED}ERROR:\${NC} \$1"

}

\# === 1. ПРОВЕРКА НА ROOT ===

print_info "Запуск установщика 'foreground' сервиса..."

if \[ "\$EUID" -ne 0 \]; then

&nbsp; print_error "Этот скрипт необходимо запускать с правами root."

&nbsp; echo "Пожалуйста, запустите: sudo ./install_fg_service.sh"

&nbsp; exit 1

fi

\# === 2. ИНТЕРАКТИВНЫЙ ВВОД ===

print_info "Пожалуйста, ответьте на несколько вопросов."

echo "Вы можете нажать Enter, чтобы использовать значения по умолчанию (в скобках)."

\# -e позволяет использовать автодополнение, -i задает значение по умолчанию

read -e -i "fp.service" -p "  1. Введите имя для .service файла: " SERVICE_NAME

read -e -i "master" -p "  2. Введите имя пользователя (User): " SERVICE_USER

read -e -i "sudo" -p "  3. Введите имя группы (Group): " SERVICE_GROUP

read -e -i "/DATASET/project/scada/faceplate" -p "  4. Введите полный путь к папке Faceplate: " FACEPLATE_PATH

\# Убираем / в конце, если он есть

FACEPLATE_PATH=\${FACEPLATE_PATH%/}

\# === 3. ВАЛИДАЦИЯ ===

print_info "Проверка введенных данных..."

\# Проверяем, что бинарник существует

EXEC_FILE="\$FACEPLATE_PATH/bin/faceplate"

if \[ ! -f "\$EXEC_FILE" \]; then

&nbsp;   print_error "Бинарник НЕ НАЙДЕН по пути: \$EXEC_FILE"

&nbsp;   echo "Пожалуйста, проверьте путь и попробуйте снова."

&nbsp;   exit 1

else

&nbsp;   print_success "Бинарник найден: \$EXEC_FILE"

fi

\# Проверяем пользователя и группу

if ! id "\$SERVICE_USER" &>/dev/null; then

&nbsp;   print_error "Пользователь '\$SERVICE_USER' не существует в системе."

&nbsp;   exit 1

fi

if ! getent group "\$SERVICE_GROUP" &>/dev/null; then

&nbsp;   print_error "Группа '\$SERVICE_GROUP' не существует в системе."

&nbsp;   exit 1

fi

print_success "Пользователь и группа существуют."

\# === 4. ГЕНЕРАЦИЯ ФАЙЛА ===

\# Путь назначения

SERVICE_DEST_FILE="/etc/systemd/system/\$SERVICE_NAME"

print_info "Подготовка unit-файла..."

\# Генерируем содержимое .service файла в переменную

SERVICE_CONTENT=\$(cat <<EOF

\[Unit\]

Description=Faceplate SCADA Application Service (Foreground)

After=network.target

\[Service\]

\# !! Процесс запускается в foreground

Type=simple

\# === ОТКЛЮЧЕНИЕ ЛОГОВ (ЭКОНОМИЯ ДИСКА) ===

StandardOutput=null

StandardError=null

\# =========================================

User=\$SERVICE_USER

Group=\$SERVICE_GROUP

WorkingDirectory=\$FACEPLATE_PATH

\# !! Запускаем бинарник напрямую

ExecStart=\$EXEC_FILE foreground

Restart=always

RestartSec=5

LimitNOFILE=200000

\[Install\]

WantedBy=multi-user.target

EOF

)

\# === 5. ПОДТВЕРЖДЕНИЕ ===

echo -e "\\n\${YELLOW}--- ПРОВЕРЬТЕ ДАННЫЕ ---\${NC}"

echo "Сервис будет установлен сюда: \${GREEN}\$SERVICE_DEST_FILE\${NC}"

echo "Содержимое файла:"

echo -e "\${BLUE}---------------------------------\${NC}"

echo "\$SERVICE_CONTENT"

echo -e "\${BLUE}---------------------------------\${NC}"

echo ""

read -p "Все верно? (y/n): " confirm

if \[ "\$confirm" != "y" \] && \[ "\$confirm" != "Y" \]; then

&nbsp;   echo "Установка отменена."

&nbsp;   exit 0

fi

\# === 6. УСТАНОВКА ===

print_info "Остановка старого сервиса (если он был)..."

systemctl stop "\$SERVICE_NAME" &>/dev/null

print_info "1. Запись файла в \$SERVICE_DEST_FILE..."

echo -e "\$SERVICE_CONTENT" > "\$SERVICE_DEST_FILE"

print_info "2. Перезагрузка демонов systemd (daemon-reload)..."

systemctl daemon-reload

print_info "3. Включение автозагрузки сервиса (enable)..."

systemctl enable "\$SERVICE_NAME"

print_info "4. Запуск сервиса \$SERVICE_NAME (start)..."

systemctl start "\$SERVICE_NAME"

\# === 7. ФИНАЛ ===

print_success "Установка и запуск завершены!"

echo "Проверка статуса сервиса (через 2 секунды):"

sleep 2

systemctl status "\$SERVICE_NAME"

exit 0
```
**В случае восстановления** необходимо запустить **Faceplate** в режиме **Force**.  
Для этого в консоли запускается **первая нода** согласно пункту 4, остальные ноды запускаются **в режиме сервиса**.  
После завершения восстановления первую ноду необходимо **остановить в консоли** и **перевести в режим сервиса**.